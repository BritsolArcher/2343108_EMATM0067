{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "# Text Analytics Coursework -- Data Loader\n",
    "\n",
    "This notebook is to help get you started with the datasets used in the coursework assignment. \n",
    "\n",
    "For this coursework, we recommend that you use your virtual environment that you created for the labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the Emotion dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e90e82-bbdb-4532-91b1-58229d4f1043",
   "metadata": {},
   "source": [
    "# Social Media Emotion Classification\n",
    "\n",
    "The dataset classifies Tweets into anger, joy, optimism or sadness.\n",
    "\n",
    "First we need to load the data. The data is already split into train, validation and test. The _validation_ set (also called 'development' set or 'devset') can be used to compute performance of your model when tuning hyperparameters, optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set (i.e., not to look at it at all when developing your method) to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specific examples in the test set. An alternative approach to validation is to not use a single fixed validation set, but instead use [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efaf1096-acce-4226-a172-5357f49e91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 3257 instances loaded\n",
      "Development/validation dataset with 374 instances loaded\n",
      "Test dataset with 1421 instances loaded\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Development/validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Access the input text and target labels like this...\n",
    "\n",
    "train_texts = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "\n",
    "val_texts = val_dataset['text']\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "test_texts = test_dataset['text']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04efd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6b1836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textsets_to_csv(textsets, labels):\n",
    "    emotion = {\n",
    "        0: \"Anger\",\n",
    "        1: \"Joy\",\n",
    "        2: \"Optimism\",\n",
    "        3: \"Sadness\"\n",
    "    }\n",
    "    data = pd.DataFrame()\n",
    "    data[\"text\"] = textsets\n",
    "    data[\"label\"] = labels\n",
    "    data[\"emotion\"] = data[\"label\"].apply(lambda x: emotion[x])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac932456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = textsets_to_csv(train_texts, train_labels)\n",
    "train.to_csv(\"../data/train.csv\",index=False)\n",
    "\n",
    "val = textsets_to_csv(val_texts, val_labels)\n",
    "val.to_csv(\"../data/val.csv\",index=False)\n",
    "\n",
    "test = textsets_to_csv(test_texts, test_labels)\n",
    "test.to_csv(\"../data/test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eb3f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babbd130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>I get discouraged because I try for 5 fucking ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>The @user are in contention and hosting @user ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>@user @user @user @user @user as a fellow UP g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>You have a #problem? Yes! Can you do #somethin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>@user @user i will fight this guy! Don't insul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3257 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     “Worry is a down payment on a problem you may ...      2\n",
       "1     My roommate: it's okay that we can't spell bec...      0\n",
       "2     No but that's so cute. Atsu was probably shy a...      1\n",
       "3     Rooneys fucking untouchable isn't he? Been fuc...      0\n",
       "4     it's pretty depressing when u hit pan on ur fa...      3\n",
       "...                                                 ...    ...\n",
       "3252  I get discouraged because I try for 5 fucking ...      3\n",
       "3253  The @user are in contention and hosting @user ...      3\n",
       "3254  @user @user @user @user @user as a fellow UP g...      0\n",
       "3255  You have a #problem? Yes! Can you do #somethin...      0\n",
       "3256  @user @user i will fight this guy! Don't insul...      0\n",
       "\n",
       "[3257 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031f8b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af15961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " \"My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\",\n",
       " \"No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu\",\n",
       " \"Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight\",\n",
       " \"it's pretty depressing when u hit pan on ur favourite highlighter\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af0fbc-2c18-4b68-839d-802595dac600",
   "metadata": {},
   "source": [
    "# Bio Creative V\n",
    "\n",
    "Marks chemicals and diseases in Pubmed articles as named entities. For further details, see [the HuggingFace page](https://huggingface.co/datasets/tner/bc5cdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d0376c-93d1-4ee9-a7ba-76bba2afc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cc995227ea4ba9a5e554b1edb468b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964c2860843c4ccea9278ba3dac7fca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57512038b48e441bb1d9fd107d0b2bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d9018cedd343a1a4bfe25e6a3e21d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868b28e1ec174880afaa0542dbf4e287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da01e222fb3415f89fae76bbd2d0282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7654018a497b4556a31917bbc522acf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb41b7c7a3a44c18973af0da1697496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06f74a723ba4d8595743a9fc8480f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa69152a0ad4cacb43f9b48689a1da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m ner_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtner/bc5cdr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dataset is a dictionary with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m splits: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "ner_dataset = load_dataset(\n",
    "    \"tner/bc5cdr\", \n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515ff00-f8bc-42ac-801f-50224ea50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It  may be useful to obtain the data in a list format for some sequence tagging methods\n",
    "train_sentences_ner = [item['tokens'] for item in ner_dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in ner_dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in ner_dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in ner_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different tag values in the dataset:\n",
    "np.unique(np.concatenate(train_labels_ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9fd33",
   "metadata": {},
   "source": [
    "### (Optional) Transformer Sequence Tagger\n",
    "\n",
    "People that want to use a transformer for task 2 may want to take a look at the [Token Classification tutorial](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=vc0BSBLIIrJQ) from HuggingFace. There is no requirement to use a transformer to achieve high marks, this is one option that you may consider. Feel free to skip this part of the notebook if you are using a different kind of model that does not require it.\n",
    "\n",
    "A useful function provided by HuggingFace as part of the Token Classification page is tokenize_and_align. You can reuse this function if you are working with a method that tokenizes the text in a diferent way to the Bio Creative V dataset. This function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07deb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, max_length=128, is_split_into_words=True)\n",
    "    print(tokenized_inputs.keys())\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# An example of how to use tokenize_and_align:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\") \n",
    "label_all_tokens=False\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b3919",
   "metadata": {},
   "source": [
    "When we get the predictions from the transformer sequence tagger, we will need to skip tokens with a training label of -100, as these are parts of a word or special tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2eddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
